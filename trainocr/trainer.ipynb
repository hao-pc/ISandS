{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:19:23.488642Z",
     "start_time": "2021-07-23T04:19:21.854534Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:19:23.885144Z",
     "start_time": "2021-07-23T04:19:23.880564Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "##### https://github.com/githubharald/CTCDecoder/blob/master/src/BeamSearch.py\n",
    "class BeamEntry:\n",
    "    \"information about one single beam at specific time-step\"\n",
    "    def __init__(self):\n",
    "        self.prTotal = 0 # blank and non-blank\n",
    "        self.prNonBlank = 0 # non-blank\n",
    "        self.prBlank = 0 # blank\n",
    "        self.prText = 1 # LM score\n",
    "        self.lmApplied = False # flag if LM was already applied to this beam\n",
    "        self.labeling = () # beam-labeling\n",
    "\n",
    "class BeamState:\n",
    "    \"information about the beams at specific time-step\"\n",
    "    def __init__(self):\n",
    "        self.entries = {}\n",
    "\n",
    "    def norm(self):\n",
    "        \"length-normalise LM score\"\n",
    "        for (k, _) in self.entries.items():\n",
    "            labelingLen = len(self.entries[k].labeling)\n",
    "            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))\n",
    "\n",
    "    def sort(self):\n",
    "        \"return beam-labelings, sorted by probability\"\n",
    "        beams = [v for (_, v) in self.entries.items()]\n",
    "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n",
    "        return [x.labeling for x in sortedBeams]\n",
    "\n",
    "    def wordsearch(self, classes, ignore_idx, beamWidth, dict_list):\n",
    "        beams = [v for (_, v) in self.entries.items()]\n",
    "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)[:beamWidth]\n",
    "\n",
    "        for j, candidate in enumerate(sortedBeams):\n",
    "            idx_list = candidate.labeling\n",
    "            text = ''\n",
    "            for i,l in enumerate(idx_list):\n",
    "                if l not in ignore_idx and (not (i > 0 and idx_list[i - 1] == idx_list[i])):  # removing repeated characters and blank.\n",
    "                    text += classes[l]\n",
    "\n",
    "            if j == 0: best_text = text\n",
    "            if text in dict_list:\n",
    "                print('found text: ', text)\n",
    "                best_text = text\n",
    "                break\n",
    "            else:\n",
    "                print('not in dict: ', text)\n",
    "        return best_text\n",
    "\n",
    "def applyLM(parentBeam, childBeam, classes, lm):\n",
    "    \"calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars\"\n",
    "    if lm and not childBeam.lmApplied:\n",
    "        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n",
    "        c2 = classes[childBeam.labeling[-1]] # second char\n",
    "        lmFactor = 0.01 # influence of language model\n",
    "        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n",
    "        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n",
    "        childBeam.lmApplied = True # only apply LM once per beam entry\n",
    "\n",
    "def addBeam(beamState, labeling):\n",
    "    \"add beam if it does not yet exist\"\n",
    "    if labeling not in beamState.entries:\n",
    "        beamState.entries[labeling] = BeamEntry()\n",
    "\n",
    "def ctcBeamSearch(mat, classes, ignore_idx, lm, beamWidth=25, dict_list = []):\n",
    "    \"beam search as described by the paper of Hwang et al. and the paper of Graves et al.\"\n",
    "\n",
    "    #blankIdx = len(classes)\n",
    "    blankIdx = 0\n",
    "    maxT, maxC = mat.shape\n",
    "\n",
    "    # initialise beam state\n",
    "    last = BeamState()\n",
    "    labeling = ()\n",
    "    last.entries[labeling] = BeamEntry()\n",
    "    last.entries[labeling].prBlank = 1\n",
    "    last.entries[labeling].prTotal = 1\n",
    "\n",
    "    # go over all time-steps\n",
    "    for t in range(maxT):\n",
    "        curr = BeamState()\n",
    "\n",
    "        # get beam-labelings of best beams\n",
    "        bestLabelings = last.sort()[0:beamWidth]\n",
    "\n",
    "        # go over best beams\n",
    "        for labeling in bestLabelings:\n",
    "\n",
    "            # probability of paths ending with a non-blank\n",
    "            prNonBlank = 0\n",
    "            # in case of non-empty beam\n",
    "            if labeling:\n",
    "                # probability of paths with repeated last char at the end\n",
    "                prNonBlank = last.entries[labeling].prNonBlank * mat[t, labeling[-1]]\n",
    "\n",
    "            # probability of paths ending with a blank\n",
    "            prBlank = (last.entries[labeling].prTotal) * mat[t, blankIdx]\n",
    "\n",
    "            # add beam at current time-step if needed\n",
    "            addBeam(curr, labeling)\n",
    "\n",
    "            # fill in data\n",
    "            curr.entries[labeling].labeling = labeling\n",
    "            curr.entries[labeling].prNonBlank += prNonBlank\n",
    "            curr.entries[labeling].prBlank += prBlank\n",
    "            curr.entries[labeling].prTotal += prBlank + prNonBlank\n",
    "            curr.entries[labeling].prText = last.entries[labeling].prText # beam-labeling not changed, therefore also LM score unchanged from\n",
    "            curr.entries[labeling].lmApplied = True # LM already applied at previous time-step for this beam-labeling\n",
    "\n",
    "            # extend current beam-labeling\n",
    "            for c in range(maxC - 1):\n",
    "                # add new char to current beam-labeling\n",
    "                newLabeling = labeling + (c,)\n",
    "\n",
    "                # if new labeling contains duplicate char at the end, only consider paths ending with a blank\n",
    "                if labeling and labeling[-1] == c:\n",
    "                    prNonBlank = mat[t, c] * last.entries[labeling].prBlank\n",
    "                else:\n",
    "                    prNonBlank = mat[t, c] * last.entries[labeling].prTotal\n",
    "\n",
    "                # add beam at current time-step if needed\n",
    "                addBeam(curr, newLabeling)\n",
    "\n",
    "                # fill in data\n",
    "                curr.entries[newLabeling].labeling = newLabeling\n",
    "                curr.entries[newLabeling].prNonBlank += prNonBlank\n",
    "                curr.entries[newLabeling].prTotal += prNonBlank\n",
    "\n",
    "                # apply LM\n",
    "                #applyLM(curr.entries[labeling], curr.entries[newLabeling], classes, lm)\n",
    "\n",
    "        # set new beam state\n",
    "        last = curr\n",
    "\n",
    "    # normalise LM scores according to beam-labeling-length\n",
    "    last.norm()\n",
    "\n",
    "    # sort by probability\n",
    "    #bestLabeling = last.sort()[0] # get most probable labeling\n",
    "\n",
    "    # map labels to chars\n",
    "    #res = ''\n",
    "    #for idx,l in enumerate(bestLabeling):\n",
    "    #    if l not in ignore_idx and (not (idx > 0 and bestLabeling[idx - 1] == bestLabeling[idx])):  # removing repeated characters and blank.\n",
    "    #        res += classes[l]\n",
    "\n",
    "    if dict_list == []:\n",
    "        bestLabeling = last.sort()[0] # get most probable labeling\n",
    "        res = ''\n",
    "        for i,l in enumerate(bestLabeling):\n",
    "            if l not in ignore_idx and (not (i > 0 and bestLabeling[i - 1] == bestLabeling[i])):  # removing repeated characters and blank.\n",
    "                res += classes[l]\n",
    "    else:\n",
    "        res = last.wordsearch(classes, ignore_idx, beamWidth, dict_list)\n",
    "\n",
    "    return res\n",
    "#####\n",
    "\n",
    "def consecutive(data, mode ='first', stepsize=1):\n",
    "    group = np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    group = [item for item in group if len(item)>0]\n",
    "\n",
    "    if mode == 'first': result = [l[0] for l in group]\n",
    "    elif mode == 'last': result = [l[-1] for l in group]\n",
    "    return result\n",
    "\n",
    "def word_segmentation(mat, separator_idx =  {'th': [1,2],'en': [3,4]}, separator_idx_list = [1,2,3,4]):\n",
    "    result = []\n",
    "    sep_list = []\n",
    "    start_idx = 0\n",
    "    for sep_idx in separator_idx_list:\n",
    "        if sep_idx % 2 == 0: mode ='first'\n",
    "        else: mode ='last'\n",
    "        a = consecutive( np.argwhere(mat == sep_idx).flatten(), mode)\n",
    "        new_sep = [ [item, sep_idx] for item in a]\n",
    "        sep_list += new_sep\n",
    "    sep_list = sorted(sep_list, key=lambda x: x[0])\n",
    "\n",
    "    for sep in sep_list:\n",
    "        for lang in separator_idx.keys():\n",
    "            if sep[1] == separator_idx[lang][0]: # start lang\n",
    "                sep_lang = lang\n",
    "                sep_start_idx = sep[0]\n",
    "            elif sep[1] == separator_idx[lang][1]: # end lang\n",
    "                if sep_lang == lang: # check if last entry if the same start lang\n",
    "                    new_sep_pair = [lang, [sep_start_idx+1, sep[0]-1]]\n",
    "                    if sep_start_idx > start_idx:\n",
    "                        result.append( ['', [start_idx, sep_start_idx-1] ] )\n",
    "                    start_idx = sep[0]+1\n",
    "                    result.append(new_sep_pair)\n",
    "                else: # reset\n",
    "                    sep_lang = ''\n",
    "\n",
    "    if start_idx <= len(mat)-1:\n",
    "        result.append( ['', [start_idx, len(mat)-1] ] )\n",
    "    return result\n",
    "\n",
    "class CTCLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    #def __init__(self, character, separator = []):\n",
    "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
    "        # character (str): set of the possible characters.\n",
    "        dict_character = list(character)\n",
    "\n",
    "        #special_character = ['\\xa2', '\\xa3', '\\xa4','\\xa5']\n",
    "        #self.separator_char = special_character[:len(separator)]\n",
    "\n",
    "        self.dict = {}\n",
    "        #for i, char in enumerate(self.separator_char + dict_character):\n",
    "        for i, char in enumerate(dict_character):\n",
    "            # NOTE: 0 is reserved for 'blank' token required by CTCLoss\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        #self.character = ['[blank]']+ self.separator_char + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        self.separator_list = separator_list\n",
    "\n",
    "        separator_char = []\n",
    "        for lang, sep in separator_list.items():\n",
    "            separator_char += sep\n",
    "\n",
    "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
    "\n",
    "        dict_list = {}\n",
    "        for lang, dict_path in dict_pathlist.items():\n",
    "            with open(dict_path, \"rb\") as input_file:\n",
    "                word_count = pickle.load(input_file)\n",
    "            dict_list[lang] = word_count\n",
    "        self.dict_list = dict_list\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\"convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "\n",
    "        output:\n",
    "            text: concatenated text index for CTCLoss.\n",
    "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
    "            length: length of each text. [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) for s in text]\n",
    "        text = ''.join(text)\n",
    "        text = [self.dict[char] for char in text]\n",
    "        text = torch.IntTensor(text).to(device)  # Перемещаем на GPU\n",
    "        length = torch.IntTensor(length).to(device)  # Перемещаем на GPU\n",
    "        return text, length\n",
    "\n",
    "    def decode_greedy(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        index = 0\n",
    "        for l in length:\n",
    "            t = text_index[index:index + l]\n",
    "\n",
    "            char_list = []\n",
    "            for i in range(l):\n",
    "                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                #if (t[i] != 0) and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                    char_list.append(self.character[t[i]])\n",
    "            text = ''.join(char_list)\n",
    "\n",
    "            texts.append(text)\n",
    "            index += l\n",
    "        return texts\n",
    "\n",
    "    def decode_beamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(mat.shape[0]):\n",
    "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
    "            texts.append(t)\n",
    "        return texts\n",
    "\n",
    "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "        argmax = np.argmax(mat, axis = 2)\n",
    "        for i in range(mat.shape[0]):\n",
    "            words = word_segmentation(argmax[i])\n",
    "            string = ''\n",
    "            for word in words:\n",
    "                matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
    "                if word[0] == '': dict_list = []\n",
    "                else: dict_list = self.dict_list[word[0]]\n",
    "                t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
    "                string += t\n",
    "            texts.append(string)\n",
    "        return texts\n",
    "\n",
    "class AttnLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
    "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
    "        list_character = list(character)\n",
    "        self.character = list_token + list_character\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(self.character):\n",
    "            # print(i, char)\n",
    "            self.dict[char] = i\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\" convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "\n",
    "        output:\n",
    "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
    "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
    "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
    "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
    "        batch_max_length += 1\n",
    "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
    "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text.append('[s]')\n",
    "            text = [self.dict[char] for char in text]\n",
    "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
    "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "\n",
    "\n",
    "class Averager(object):\n",
    "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, v):\n",
    "        count = v.data.numel()\n",
    "        v = v.data.sum()\n",
    "        self.n_count += count\n",
    "        self.sum += v\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_count = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def val(self):\n",
    "        res = 0\n",
    "        if self.n_count != 0:\n",
    "            res = self.sum / float(self.n_count)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torch\n",
    "import pandas  as pd\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "# from torch._utils import _accumulate\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def contrast_grey(img):\n",
    "    high = np.percentile(img, 90)\n",
    "    low  = np.percentile(img, 10)\n",
    "    return (high-low)/(high+low), high, low\n",
    "\n",
    "def adjust_contrast_grey(img, target = 0.4):\n",
    "    contrast, high, low = contrast_grey(img)\n",
    "    if contrast < target:\n",
    "        img = img.astype(int)\n",
    "        ratio = 200./(high-low)\n",
    "        img = (img - low + 25)*ratio\n",
    "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def _accumulate(iterable, fn=lambda x, y: x + y):\n",
    "    \"Return running totals\"\n",
    "    # _accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n",
    "    # _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n",
    "    it = iter(iterable)\n",
    "    try:\n",
    "        total = next(it)\n",
    "    except StopIteration:\n",
    "        return\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = fn(total, element)\n",
    "        yield total\n",
    "\n",
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image, text = next(data_loader_iter)\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = next(self.dataloader_iter_list[i])\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts\n",
    "\n",
    "\n",
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = OCRDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root, opt):\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        print(root)\n",
    "        self.df = pd.read_csv(os.path.join(root, 'labels.csv'), sep=',', engine='python', names=['filename', 'words'], index_col=False, keep_default_na=False)\n",
    "        \n",
    "        # Заполнение пустых значений\n",
    "        self.df['words'] = self.df['words'].fillna('')\n",
    "        \n",
    "        self.nSamples = len(self.df)\n",
    "\n",
    "        if self.opt.data_filtering_off:\n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "        else:\n",
    "            self.filtered_index_list = []\n",
    "            for index in range(self.nSamples):\n",
    "                label = self.df.at[index, 'words']\n",
    "                \n",
    "                # Проверка на None и пустые строки\n",
    "                if label is None or label == '':\n",
    "                    print(f\"Warning: Empty label at index {index}, filename: {self.df.at[index, 'filename']}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(label)\n",
    "                \n",
    "                out_of_char = f'[^{self.opt.character}]'\n",
    "                if re.search(out_of_char, label.lower()):\n",
    "                    continue\n",
    "                \n",
    "                self.filtered_index_list.append(index)\n",
    "            self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.filtered_index_list[index]  # - 1\n",
    "        img_fname = self.df.at[index, 'filename']\n",
    "        img_fpath = os.path.join(self.root, img_fname)\n",
    "        label = self.df.at[index, 'words']\n",
    "\n",
    "        if self.opt.rgb:\n",
    "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
    "        else:\n",
    "            img = Image.open(img_fpath).convert('L')\n",
    "\n",
    "        if not self.opt.sensitive:\n",
    "            label = label.lower()\n",
    "\n",
    "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "        out_of_char = f'[^{self.opt.character}]'\n",
    "        label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.contrast_adjust = contrast_adjust\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "\n",
    "                #### augmentation here - change contrast\n",
    "                if self.contrast_adjust > 0:\n",
    "                    image = np.array(image.convert(\"L\"))\n",
    "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
    "                    image = Image.fromarray(image, 'L')\n",
    "\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    image_numpy = image_tensor.cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn as nn\n",
    "from modules.transformation import TPS_SpatialTransformerNetwork\n",
    "from modules.feature_extraction import VGG_FeatureExtractor, RCNN_FeatureExtractor, ResNet_FeatureExtractor\n",
    "from modules.sequence_modeling import BidirectionalLSTM\n",
    "from modules.prediction import Attention\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        super(Model, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,\n",
    "                       'Seq': opt.SequenceModeling, 'Pred': opt.Prediction}\n",
    "\n",
    "        \"\"\" Transformation \"\"\"\n",
    "        if opt.Transformation == 'TPS':\n",
    "            self.Transformation = TPS_SpatialTransformerNetwork(\n",
    "                F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)\n",
    "        else:\n",
    "            print('No Transformation module specified')\n",
    "\n",
    "        \"\"\" FeatureExtraction \"\"\"\n",
    "        if opt.FeatureExtraction == 'VGG':\n",
    "            self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        elif opt.FeatureExtraction == 'RCNN':\n",
    "            self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        elif opt.FeatureExtraction == 'ResNet':\n",
    "            self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        else:\n",
    "            raise Exception('No FeatureExtraction module specified')\n",
    "        self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
    "\n",
    "        \"\"\" Sequence modeling\"\"\"\n",
    "        if opt.SequenceModeling == 'BiLSTM':\n",
    "            self.SequenceModeling = nn.Sequential(\n",
    "                BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),\n",
    "                BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))\n",
    "            self.SequenceModeling_output = opt.hidden_size\n",
    "        else:\n",
    "            print('No SequenceModeling module specified')\n",
    "            self.SequenceModeling_output = self.FeatureExtraction_output\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        if opt.Prediction == 'CTC':\n",
    "            self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
    "        elif opt.Prediction == 'Attn':\n",
    "            self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)\n",
    "        else:\n",
    "            raise Exception('Prediction is neither CTC or Attn')\n",
    "\n",
    "    def forward(self, input, text, is_train=True):\n",
    "        \"\"\" Transformation stage \"\"\"\n",
    "        if not self.stages['Trans'] == \"None\":\n",
    "            input = self.Transformation(input)\n",
    "\n",
    "        \"\"\" Feature extraction stage \"\"\"\n",
    "        visual_feature = self.FeatureExtraction(input)\n",
    "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
    "        visual_feature = visual_feature.squeeze(3)\n",
    "\n",
    "        \"\"\" Sequence modeling stage \"\"\"\n",
    "        if self.stages['Seq'] == 'BiLSTM':\n",
    "            contextual_feature = self.SequenceModeling(visual_feature)\n",
    "        else:\n",
    "            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
    "\n",
    "        \"\"\" Prediction stage \"\"\"\n",
    "        if self.stages['Pred'] == 'CTC':\n",
    "            prediction = self.Prediction(contextual_feature.contiguous())\n",
    "        else:\n",
    "            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def validation(model, criterion, evaluation_loader, converter, opt, device):\n",
    "    \"\"\" validation or evaluation \"\"\"\n",
    "    n_correct = 0\n",
    "    norm_ED = 0\n",
    "    length_of_data = 0\n",
    "    infer_time = 0\n",
    "    valid_loss_avg = Averager()\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        length_of_data = length_of_data + batch_size\n",
    "        image = image_tensors.to(device)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "        text_for_pred = text_for_pred.to(device)\n",
    "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        text_for_loss = text_for_loss.to(device)\n",
    "        length_for_loss = length_for_loss.to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text_for_pred)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            # Calculate evaluation loss for CTC decoder.\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            preds_size = preds_size.to(device)\n",
    "            # permute 'preds' to use CTCloss format\n",
    "            cost = criterion(preds.log_softmax(2).permute(1, 0, 2), text_for_loss, preds_size, length_for_loss)\n",
    "\n",
    "            if opt.decode == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "            elif opt.decode == 'beamsearch':\n",
    "                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n",
    "\n",
    "        else:\n",
    "            preds = model(image, text_for_pred, is_train=False)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
    "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
    "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
    "\n",
    "        infer_time += forward_time\n",
    "        valid_loss_avg.add(cost)\n",
    "\n",
    "        # calculate accuracy & confidence score\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        confidence_score_list = []\n",
    "        \n",
    "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                gt = gt[:gt.find('[s]')]\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            if pred == gt:\n",
    "                n_correct += 1\n",
    "\n",
    "            '''\n",
    "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
    "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n",
    "            if len(gt) == 0:\n",
    "                norm_ED += 1\n",
    "            else:\n",
    "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
    "            '''\n",
    "            \n",
    "            # ICDAR2019 Normalized Edit Distance \n",
    "            if len(gt) == 0 or len(pred) ==0:\n",
    "                norm_ED += 0\n",
    "            elif len(gt) > len(pred):\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "            else:\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            try:\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            except:\n",
    "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
    "            confidence_score_list.append(confidence_score)\n",
    "            # print(pred, gt, pred==gt, confidence_score)\n",
    "\n",
    "    accuracy = n_correct / float(length_of_data) * 100\n",
    "    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n",
    "\n",
    "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    print(\"Modules, Parameters\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        #table.add_row([name, param])\n",
    "        total_params+=param\n",
    "        print(name, param)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "def train(opt, show_number = 2, amp=False):\n",
    "    \"\"\" dataset preparation \"\"\"\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "\n",
    "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf-8-sig\")\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(opt.workers), # prefetch_factor=None,\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "    log.write(valid_dataset_log)\n",
    "    print('-' * 80)\n",
    "    log.write('-' * 80 + '\\n')\n",
    "    log.close()\n",
    "    \n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        pretrained_dict = torch.load(opt.saved_model)\n",
    "        if opt.new_prediction:\n",
    "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n",
    "        \n",
    "        model = torch.nn.DataParallel(model).to(device) \n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(pretrained_dict, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(pretrained_dict)\n",
    "        if opt.new_prediction:\n",
    "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n",
    "            for name, param in model.module.Prediction.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            model = model.to(device) \n",
    "    else:\n",
    "        # weight initialization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'localization_fc2' in name:\n",
    "                print(f'Skip {name} as it is already initialized')\n",
    "                continue\n",
    "            try:\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            except Exception as e:  # for batchnorm.\n",
    "                if 'weight' in name:\n",
    "                    param.data.fill_(1)\n",
    "                continue\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    \n",
    "    model.train() \n",
    "    print(\"Model:\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    \"\"\" setup loss \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
    "    # loss averager\n",
    "    loss_avg = Averager()\n",
    "\n",
    "    # freeze some layers\n",
    "    try:\n",
    "        if opt.freeze_FeatureFxtraction:\n",
    "            for param in model.module.FeatureExtraction.parameters():\n",
    "                param.requires_grad = False\n",
    "        if opt.freeze_SequenceModeling:\n",
    "            for param in model.module.SequenceModeling.parameters():\n",
    "                param.requires_grad = False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # filter that only require gradient decent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params num : ', sum(params_num))\n",
    "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
    "\n",
    "    # setup optimizer\n",
    "    if opt.optim=='adam':\n",
    "        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        optimizer = optim.Adam(filtered_parameters)\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "    print(\"Optimizer:\")\n",
    "    print(optimizer)\n",
    "\n",
    "    \"\"\" final options \"\"\"\n",
    "    # print(opt)\n",
    "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf-8-sig\") as opt_file:\n",
    "        opt_log = '------------ Options -------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "        opt_log += '---------------------------------------\\n'\n",
    "        print(opt_log)\n",
    "        opt_file.write(opt_log)\n",
    "\n",
    "    \"\"\" start training \"\"\"\n",
    "    start_iter = 0\n",
    "    if opt.saved_model != '':\n",
    "        try:\n",
    "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "            print(f'continue to train, start_iter: {start_iter}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    i = start_iter\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    t1= time.time()\n",
    "        \n",
    "    while(True):\n",
    "        # train part\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if amp:\n",
    "            with autocast():\n",
    "                image_tensors, labels = train_dataset.get_batch()\n",
    "                image = image_tensors.to(device)\n",
    "                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "                batch_size = image.size(0)\n",
    "\n",
    "                if 'CTC' in opt.Prediction:\n",
    "                    preds = model(image, text).log_softmax(2)\n",
    "                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                    preds = preds.permute(1, 0, 2)\n",
    "                    torch.backends.cudnn.enabled = False\n",
    "                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                    torch.backends.cudnn.enabled = True\n",
    "                else:\n",
    "                    preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                    target = text[:, 1:]  # without [GO] Symbol\n",
    "                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            scaler.scale(cost).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            image_tensors, labels = train_dataset.get_batch()\n",
    "            image = image_tensors.to(device)\n",
    "            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "            batch_size = image.size(0)\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text).log_softmax(2)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                preds = preds.permute(1, 0, 2)\n",
    "                torch.backends.cudnn.enabled = False\n",
    "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                torch.backends.cudnn.enabled = True\n",
    "            else:\n",
    "                preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                target = text[:, 1:]  # without [GO] Symbol\n",
    "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            cost.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n",
    "            optimizer.step()\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (i % opt.valInterval == 0) and (i!=0):\n",
    "            print('training time: ', time.time()-t1)\n",
    "            t1=time.time()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # for log\n",
    "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf-8-sig\") as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n",
    "                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                model.train()\n",
    "\n",
    "                # training loss and validation loss\n",
    "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
    "\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                \n",
    "                #show_number = min(show_number, len(labels))\n",
    "                \n",
    "                start = random.randint(0,len(labels) - show_number )    \n",
    "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "                print('validation time: ', time.time()-t1)\n",
    "                t1=time.time()\n",
    "        # save model per 1e+4 iter.\n",
    "        if (i + 1) % 1e+4 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
    "\n",
    "        if i == opt.num_iter:\n",
    "            print('end the training')\n",
    "            sys.exit()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:19:24.119144Z",
     "start_time": "2021-07-23T04:19:24.112032Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8-sig\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep=',', engine='python', names=['filename', 'words'], index_col=False, keep_default_na=False)\n",
    "            df['words'] = df['words'].fillna('')\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:49:07.045060Z",
     "start_time": "2021-07-23T04:20:15.050992Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['ru_train_filtered']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: ru_train_filtered\n",
      "all_data/ru_train_filtered\n",
      "sub-directory:\t/ru_train_filtered\t num samples: 510\n",
      "num total samples of ru_train_filtered: 510 x 1.0 (total_data_usage_ratio) = 510\n",
      "num samples of ru_train_filtered per batch: 64 x 1.0 (batch_ratio) = 64\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 64 = 64\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data/val\t dataset: /\n",
      "all_data/val/\n",
      "sub-directory:\t/.\t num samples: 186\n",
      "--------------------------------------------------------------------------------\n",
      "No Transformation module specified\n",
      "model input parameters 64 600 20 1 256 256 208 150 None VGG BiLSTM CTC\n",
      "loading pretrained model from saved_models/ru_train_filtered/best_accuracy.pth\n",
      "Model:\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (FeatureExtraction): VGG_FeatureExtractor(\n",
      "      (ConvNet): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU(inplace=True)\n",
      "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): ReLU(inplace=True)\n",
      "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (16): ReLU(inplace=True)\n",
      "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "        (19): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "    (SequenceModeling): Sequential(\n",
      "      (0): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Prediction): Linear(in_features=256, out_features=208, bias=True)\n",
      "  )\n",
      ")\n",
      "Modules, Parameters\n",
      "module.FeatureExtraction.ConvNet.0.weight 288\n",
      "module.FeatureExtraction.ConvNet.0.bias 32\n",
      "module.FeatureExtraction.ConvNet.3.weight 18432\n",
      "module.FeatureExtraction.ConvNet.3.bias 64\n",
      "module.FeatureExtraction.ConvNet.6.weight 73728\n",
      "module.FeatureExtraction.ConvNet.6.bias 128\n",
      "module.FeatureExtraction.ConvNet.8.weight 147456\n",
      "module.FeatureExtraction.ConvNet.8.bias 128\n",
      "module.FeatureExtraction.ConvNet.11.weight 294912\n",
      "module.FeatureExtraction.ConvNet.12.weight 256\n",
      "module.FeatureExtraction.ConvNet.12.bias 256\n",
      "module.FeatureExtraction.ConvNet.14.weight 589824\n",
      "module.FeatureExtraction.ConvNet.15.weight 256\n",
      "module.FeatureExtraction.ConvNet.15.bias 256\n",
      "module.FeatureExtraction.ConvNet.18.weight 262144\n",
      "module.FeatureExtraction.ConvNet.18.bias 256\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.0.linear.weight 131072\n",
      "module.SequenceModeling.0.linear.bias 256\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.1.linear.weight 131072\n",
      "module.SequenceModeling.1.linear.bias 256\n",
      "module.Prediction.weight 53248\n",
      "module.Prediction.bias 208\n",
      "Total Trainable Params: 3809872\n",
      "Trainable params num :  3809872\n",
      "Optimizer:\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1.0\n",
      "    maximize: False\n",
      "    rho: 0.95\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "number: 0123456789\n",
      "symbol: !\"#$№%&\\()*+-./:;<=>?@[\\\\]^_`{|}~ «»₽\n",
      "lang_char: ₽ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЂђЃѓЄєІіЇїЈјЉљЊњЋћЌќЎўЏџҐґҒғҚқҮүҲҳҶҷӀӏӢӣӨ\n",
      "experiment_name: ru_train_filtered\n",
      "train_data: all_data\n",
      "valid_data: all_data/val\n",
      "manualSeed: 1111\n",
      "workers: 0\n",
      "batch_size: 64\n",
      "num_iter: 5000\n",
      "valInterval: 1000\n",
      "saved_model: saved_models/ru_train_filtered/best_accuracy.pth\n",
      "FT: True\n",
      "optim: False\n",
      "lr: 1.0\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "select_data: ['ru_train_filtered']\n",
      "batch_ratio: ['1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 150\n",
      "imgH: 64\n",
      "imgW: 600\n",
      "rgb: False\n",
      "contrast_adjust: True\n",
      "sensitive: True\n",
      "PAD: True\n",
      "data_filtering_off: False\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 256\n",
      "hidden_size: 256\n",
      "decode: greedy\n",
      "new_prediction: False\n",
      "freeze_FeatureFxtraction: False\n",
      "freeze_SequenceModeling: False\n",
      "character: 0123456789!\"#$№%&\\()*+-./:;<=>?@[\\\\]^_`{|}~ «»₽₽ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЂђЃѓЄєІіЇїЈјЉљЊњЋћЌќЎўЏџҐґҒғҚқҮүҲҳҶҷӀӏӢӣӨ\n",
      "num_class: 208\n",
      "---------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artas\\AppData\\Local\\Temp\\ipykernel_985052\\3192973644.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  200.63710975646973\n",
      "[1000/5000] Train loss: 0.00047, Valid loss: 0.52412, Elapsed_time: 200.64015\n",
      "Current_accuracy : 79.570, Current_norm_ED  : 0.9257\n",
      "Best_accuracy    : 79.570, Best_norm_ED     : 0.9257\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "04                        | 04                        | 0.9715\tTrue\n",
      "лександровна              | лександровна              | 0.9987\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  1.8796381950378418\n",
      "training time:  265.8397641181946\n",
      "[2000/5000] Train loss: 0.00021, Valid loss: 0.59013, Elapsed_time: 468.36907\n",
      "Current_accuracy : 79.570, Current_norm_ED  : 0.9257\n",
      "Best_accuracy    : 79.570, Best_norm_ED     : 0.9257\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "ХРИСТОФОРОВИЧ             | ХРИСТОФОРОВИЧ             | 0.4711\tTrue\n",
      "2027 г.                   | 2027 г.                   | 0.9939\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  2.7570178508758545\n",
      "training time:  223.2778058052063\n",
      "[3000/5000] Train loss: 0.00174, Valid loss: 0.46292, Elapsed_time: 694.40689\n",
      "Current_accuracy : 81.183, Current_norm_ED  : 0.9291\n",
      "Best_accuracy    : 81.183, Best_norm_ED     : 0.9291\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "«2»                       | «2»                       | 0.9999\tTrue\n",
      "59                        | 5о9                       | 0.4097\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  1.8201448917388916\n",
      "training time:  196.69906377792358\n",
      "[4000/5000] Train loss: 0.00019, Valid loss: 0.49476, Elapsed_time: 892.93061\n",
      "Current_accuracy : 80.645, Current_norm_ED  : 0.9332\n",
      "Best_accuracy    : 81.183, Best_norm_ED     : 0.9332\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "08                        | 09                        | 0.3842\tFalse\n",
      "59                        | 5о9                       | 0.3064\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  1.901723861694336\n",
      "training time:  197.79662990570068\n",
      "[5000/5000] Train loss: 0.00014, Valid loss: 0.52452, Elapsed_time: 1092.63197\n",
      "Current_accuracy : 81.183, Current_norm_ED  : 0.9342\n",
      "Best_accuracy    : 81.183, Best_norm_ED     : 0.9342\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "ГОР. ИЛАНСКИЙ             | ГОР. ИЛАНСКИЙ             | 0.7427\tTrue\n",
      "две тысячи тринадцатого года | две тысячи тринадцатого года | 0.7713\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  2.2704715728759766\n",
      "end the training\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artas\\PycharmProjects\\scanner\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "opt = get_config(\"config_files/ru_filtered_config.yaml\")\n",
    "train(opt, amp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
